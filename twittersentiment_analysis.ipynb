{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WLjLeslowdhw"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('/content/twitter_training.csv')\n",
        "train"
      ],
      "metadata": {
        "id": "iVlWM1C4yUDr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = pd.read_csv('/content/twitter_validation.csv')\n",
        "test"
      ],
      "metadata": {
        "id": "pr7KPAUHyl_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test.columns = ['Header1', 'company','labels','text']\n",
        "train.columns = ['Header1', 'company','labels','text']"
      ],
      "metadata": {
        "id": "h-_1OJ3bQLrR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train)\n",
        "print(\"---------------------------------------------------------------------\")\n",
        "print(test)"
      ],
      "metadata": {
        "id": "vKknWFTMQLn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.drop(columns=[\"Header1\",\"company\"],inplace=True)\n",
        "test.drop(columns=[\"Header1\",\"company\"],inplace=True)"
      ],
      "metadata": {
        "id": "gr--AEtuQLlO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.head(),test.head()"
      ],
      "metadata": {
        "id": "pO-QRxduQLiq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment=pd.concat([train,test],ignore_index=True)\n",
        "sentiment"
      ],
      "metadata": {
        "id": "MM7scYZJQLgK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment.info()"
      ],
      "metadata": {
        "id": "S-Yhs2jqQLd2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment.isnull().sum()"
      ],
      "metadata": {
        "id": "gg7gOZPuQLa8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment.dropna(inplace=True)\n",
        "sentiment.isnull().sum()"
      ],
      "metadata": {
        "id": "ywTXJrk_QLYX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment.duplicated().sum()"
      ],
      "metadata": {
        "id": "jSLOiCE_QLV-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment.drop_duplicates(inplace=True)\n",
        "sentiment.duplicated().sum()"
      ],
      "metadata": {
        "id": "lLp4_ROGQLQx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment"
      ],
      "metadata": {
        "id": "ysIdyjpZsEVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment['text'].str.len()!=0"
      ],
      "metadata": {
        "id": "Y8Fq6fS9sINK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment['text'] = sentiment['text'].astype(str)\n",
        "sentiment['text'] = sentiment['text'].str.lower()\n",
        "sentiment['text']"
      ],
      "metadata": {
        "id": "YularcUpsP_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import numpy as np\n",
        "nltk.download('wordnet', \"/kaggle/working/nltk_data/\")\n",
        "nltk.download('omw-1.4', \"/kaggle/working/nltk_data/\")\n",
        "! unzip /kaggle/working/nltk_data/corpora/wordnet.zip -d /kaggle/working/nltk_data/corpora\n",
        "! unzip /kaggle/working/nltk_data/corpora/omw-1.4.zip -d /kaggle/working/nltk_data/corpora\n",
        "nltk.data.path.append(\"/kaggle/working/nltk_data/\")"
      ],
      "metadata": {
        "id": "F0JHpCFlsm6a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_text(text):\n",
        "    text = re.sub(r'\\s+', ' ', text, flags=re.I) # Remove extra white space from text\n",
        "\n",
        "    text = re.sub(r'\\W', ' ', str(text)) # Remove all the special characters from text\n",
        "\n",
        "    text = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', text) # Remove all single characters from text\n",
        "\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text) # Remove any character that isn't alphabetical\n",
        "\n",
        "    text = text.lower()\n",
        "\n",
        "    words = word_tokenize(text)\n",
        "\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    words = [lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    Words = [word for word in words if word not in stop_words]\n",
        "\n",
        "    Words = [word for word in Words if len(word) > 3]\n",
        "\n",
        "    indices = np.unique(Words, return_index=True)[1]\n",
        "    cleaned_text = np.array(Words)[np.sort(indices)].tolist()\n",
        "\n",
        "    return cleaned_text"
      ],
      "metadata": {
        "id": "32tc8Hb0tRHH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = sentiment.text\n",
        "y = sentiment.labels\n",
        "x,y"
      ],
      "metadata": {
        "id": "SF51uRdEthTe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts = list(x)"
      ],
      "metadata": {
        "id": "twTsL__uttdv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts"
      ],
      "metadata": {
        "id": "O430hLPUukoF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "GQTHSPQ4wGWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_text = [process_text(text) for text in texts]"
      ],
      "metadata": {
        "id": "W0TOVQozuHMi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_text[:10]"
      ],
      "metadata": {
        "id": "6AxZc9f-uQMt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FvVgAybCW0ov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "624pig-32Ely"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(cleaned_text,y, test_size = 0.2, random_state =42)"
      ],
      "metadata": {
        "id": "runE0slx1hrF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "metadata": {
        "id": "dluddFMW1hnz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_vocab = 20000\n",
        "tokenizer = Tokenizer(num_words = max_vocab)\n",
        "tokenizer.fit_on_texts(x_train)\n",
        "word_idx = tokenizer.word_index\n",
        "v= len(word_idx)\n",
        "print(\"the size of vocab = \", v)\n",
        "x_train = tokenizer.texts_to_sequences(x_train)\n",
        "x_test = tokenizer.texts_to_sequences(x_test)"
      ],
      "metadata": {
        "id": "8iB1FQgR2ico"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.sequence import pad_sequences"
      ],
      "metadata": {
        "id": "p0J-_aA-5itv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "maxlen =100\n",
        "x_train = pad_sequences(x_train, maxlen = maxlen)\n",
        "x_test = pad_sequences(x_test, maxlen = maxlen)"
      ],
      "metadata": {
        "id": "vRIs7tZK5m0M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y.value_counts()"
      ],
      "metadata": {
        "id": "0vI1E7u-51il"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense,Input,GlobalMaxPooling1D,Dropout,Bidirectional\n",
        "from keras.models import Model\n",
        "D=100\n",
        "input=Input(shape=(maxlen,))"
      ],
      "metadata": {
        "id": "JCREYbpT54XI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.optimizers import Adam"
      ],
      "metadata": {
        "id": "2cdODma_7Ijl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 0.0001\n",
        "x = Embedding(v+1, D) (input)\n",
        "x= Dropout(0.5)(x)\n",
        "x = Bidirectional(LSTM(150))(x)\n",
        "x= Dense(32, activation = 'relu')(x)\n",
        "x = Dense (4, activation = 'softmax')(x)\n",
        "\n",
        "model = Model(input, x)\n",
        "\n",
        "optimizer = Adam(learning_rate = lr)\n",
        "\n",
        "model.compile(optimizer = optimizer, loss = 'categorical_crossentropy',\n",
        "              metrics = [\"acc\"])\n"
      ],
      "metadata": {
        "id": "0kex0oE47RwM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()"
      ],
      "metadata": {
        "id": "1pAjnY1k8OQr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_en = le.fit_transform(y_train)\n",
        "y_test_en = le.fit_transform(y_test)"
      ],
      "metadata": {
        "id": "Xjar9_yy8TJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_one_hot = tf.keras.utils.to_categorical(y_train_en)\n",
        "y_test_one_hot = tf.keras.utils.to_categorical(y_test_en)\n"
      ],
      "metadata": {
        "id": "SI1EQIVY8h9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(x_train, y_train_one_hot, epochs =40,\n",
        "                    validation_data = (x_test, y_test_one_hot))"
      ],
      "metadata": {
        "id": "HHJWMW0_81O2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (10,6))\n",
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title(\"Model Accuracy\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(['Train', 'Test'], loc ='upper left')\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "XLrdBe-49af-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (10,6))\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title(\"Model Loss\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(['Train', 'Test'], loc ='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wd3XTGLTo1NA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss, accuracy = model.evaluate(x_test, y_test_one_hot)\n",
        "print(\"Test Loss:\", loss)\n",
        "print(\"Test Accuracy:\", accuracy)"
      ],
      "metadata": {
        "id": "8AnieQp8DI3s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix"
      ],
      "metadata": {
        "id": "irH1kZ3mDdS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(x_test)\n",
        "y_pred_labels = np.argmax(y_pred, axis =1)\n",
        "y_true_labels = np.argmax(y_test_one_hot, axis =1)\n",
        "confusion_matrix = confusion_matrix(y_true_labels, y_pred_labels)\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.heatmap(confusion_matrix, annot =True, fmt = 'd', cmap= 'Blues',\n",
        "            xticklabels=['Class 0', 'Class 1', 'Class 2', 'Class 3'],\n",
        "            yticklabels=['Class 0', 'Class 1', 'Class 2', 'Class 3'])\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wbhGguI4DkvZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}